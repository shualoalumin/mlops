{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # 구글 드라이브 마운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade -qq git+https://github.com/huggingface/diffusers.git transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (English) Installing diffusers version 0.3.1 as you requested.\n",
    "# (한국어) 요청하신 대로 diffusers 버전 0.3.1을 설치합니다.\n",
    "! pip install --quiet diffusers==0.31.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusers version: 0.31.0\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "\n",
    "# (English) This code prints the installed version of the diffusers library.\n",
    "# (한국어) 이 코드는 diffusers 라이브러리의 설치된 버전을 출력합니다.\n",
    "print(\"Diffusers version:\", diffusers.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Image Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d1b40ef2a44de7bb6e22c4eacbbb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7978ac6806c7407c909f39d01bc73239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1b55a3f6f84d35bde2e103af442346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler%2Fscheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30b7267df3246fba3b39821ddc7e736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder%2Fconfig.json:   0%|          | 0.00/738 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33edda25644f4e14a5abcc0ec8442192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer%2Fvocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e740888e1f0f4b02b79ddcf71c2dbe1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer%2Fspecial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2a8909e47a4b87b2f6d2aafc100324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer%2Fmerges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbaacd9195944f5e86c713ca7310fe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer%2Ftokenizer_config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7501fb428cde4e119ae5dd77350b2e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf4414a44e54306a74a59b2f389868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae%2Fconfig.json:   0%|          | 0.00/716 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19112c7bc2294237820773aed539d1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11c1a53de6747fca42e121b3a3438e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0888ff0e814108ae4631da184e8932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ure_extractor%2Fpreprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aafea9ab3ca41c7af348167d9f2c4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet%2Fconfig.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d744893f7f9b46c08278c86561e9d915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m pipe \u001b[38;5;241m=\u001b[39m DiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(repo_id, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m     10\u001b[0m pipe\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m DPMSolverMultistepScheduler\u001b[38;5;241m.\u001b[39mfrom_config(pipe\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m---> 11\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:454\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb:\n\u001b[1;32m--> 454\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    457\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[0;32m    461\u001b[0m ):\n\u001b[0;32m    462\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:3110\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3109\u001b[0m         )\n\u001b[1;32m-> 3110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1324\u001b[0m             device,\n\u001b[0;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1326\u001b[0m             non_blocking,\n\u001b[0;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1328\u001b[0m         )\n\u001b[1;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# 파이프라인 불러오기\n",
    "repo_id = \"stabilityai/stable-diffusion-2-base\"\n",
    "pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 하나의 이미지 생성하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행하면 빈 칸이 표시됩니다. 위에서 작성한 프롬프트를 입력하고 엔터를 눌러보세요.  \n",
    "input() 함수를 사용하면 사용자가 입력한 값이 변수에 저장됩니다.  \n",
    "모델이 처리할 수 있는 토큰의 최대 길이는 77이므로, 너무 긴 프롬프트는 잘릴 수 있으니 적절한 길이로 작성해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = str(input('prompt: There lies a castle in the sky, where clouds drift like whispers, where the wind hums forgotten dreams. Suspended in endless blue, its spires touch the heavens, glistening in golden dawns and silver-drenched nights. No roads lead to its gates; only unburdened hearts may find the way. The weary, the dreamers, those who embrace sorrow yet choose hope—these are the travelers who glimpse its towering grace. It is not of stone nor bound by time but stands as a promise, a sanctuary of longing where all lost things are found. Laughter chimes like ancient bells, silence hums with untold stories. Rivers shimmer with distant wishes, and the air breathes melodies with no end. If you listen, you may hear your own voice echoing within, whispering the name you had before the world taught you to forget. Oh, to dwell in that celestial haven, where burdens fade like morning mist, where love stretches endless as the sky! But perhaps it is not so distant after all. Perhaps it lingers between heartbeats, waiting for the moment you close your eyes, spread your wings, and believe.'))  # 프롬프트를 영어로 입력해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 저장 폴더 만들기   \n",
    "import os\n",
    "\n",
    "\n",
    "# diffusers 폴더 생성 (구글 드라이브 내)\n",
    "save_path = '/content/drive/MyDrive/diffusers'\n",
    "os.makedirs(save_path, exist_ok=True)  # exist_ok=True로 설정하면 폴더가 이미 있어도 에러가 발생하지 않습니다\n",
    "\n",
    "# 입력한 프롬프트를 사용하여 이미지 생성 \n",
    "image = pipe(prompt, num_inference_steps=25).images[0]\n",
    "\n",
    "# 이미지 저장 (구글 드라이브에)\n",
    "image.save(f\"{save_path}/image.png\")\n",
    "\n",
    "# 이미지 출력 \n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 하이퍼파라미터를 변화시키면 더 멋진 이미지가 나올 수 있습니다.  \n",
    "\n",
    "- height, width: 생성될 이미지의 가로와 세로 픽셀 크기를 조절할 수 있습니다. 8의 배수로 설정해 주세요.  \n",
    "- num_inference_steps: denoising 스텝 수로, 값이 커질수록 고해상도 이미지가 출력되지만 출력되는 시간이 오래 걸립니다. default 값은 50입니다.\n",
    "- guidance_scale: 얼마나 주어진 프롬프트에 근접한 이미지를 생성할지를 설정하는 하이퍼파라미터로, 값이 커질수록 문자열에 근접한 이미지가 생성되지만 이미지 품질이 떨어질 수 있습니다. default는 7.5입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 설정으로 이미지를 생성해보세요\n",
    "\n",
    "new_image = pipe(\n",
    "    prompt=\"A serene lakeside sunset in watercolor style\",  # 생성하고자 하는 이미지에 대한 프롬프트\n",
    "    height=512,                                           # 이미지 높이 (8의 배수로 설정)\n",
    "    width=512,                                            # 이미지 너비 (8의 배수로 설정) \n",
    "    num_inference_steps=30,                               # 더 높은 품질을 위한 denoising 스텝 수\n",
    "    guidance_scale=7.5                                    # 프롬프트와의 유사도를 조절\n",
    ").images[0]\n",
    "\n",
    "# 생성된 이미지 저장\n",
    "new_image.save(f\"{save_path}/my_generated_image.png\")\n",
    "\n",
    "# 생성된 이미지 표시\n",
    "new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 여러 개의 이미지 생성하기  \n",
    "이번에는 하나의 프롬프트로 여러 개의 이미지를 생성해 봅시다.\n",
    "\n",
    "이미지 처리를 해주는 파이썬 라이브러리인 pillow를 사용하여 여러 개의 이미지를 담을 틀을 먼저 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 이미지 처리 라이브러리 pillow 불러오기 \n",
    "from PIL import Image  \n",
    "\n",
    "# 틀 만들기\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size  \n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box = (i%cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "틀을 만들어 주었으니 원하는 이미지를 출력해 볼까요?\n",
    "\n",
    "출력하고 싶은 이미지의 개수를 적어주고, 프롬프트를 리스트 안에 적어주세요. 아래의 코드에서는 1개의 프롬프트만 사용했지만 여러 개의 프롬프트를 사용할 수도 있어요.\n",
    "\n",
    "주의할 점은 이미지의 개수는 이미지를 담을 틀의 개수와 동일해야 한다는 것이에요. assert len(imgs) == rows * cols라는 코드에서 볼 수 있듯이, 출력하고 싶은 이미지의 개수가 6개라면 행과 열은 2와 3으로 설정해 주어야 해요. 3과 2, 1과 6도 가능하겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지의 개수 \n",
    "num_images = 6\n",
    "\n",
    "# 프롬프트 입력\n",
    "prompt = ['a horse riding a person'] * num_images\n",
    "\n",
    "# 이미지 생성\n",
    "images = pipe(prompt).images\n",
    "\n",
    "# 이미지 출력\n",
    "grid = image_grid(images, rows= 3, cols= 2)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지의 개수와 프롬프트를 변경해 가면서 다양한 이미지를 생성해 보세요.\n",
    "# Q. 다양한 이미지를 생성해 보세요.\n",
    "# 이미지의 개수 \n",
    "num_images = 9\n",
    "\n",
    "# 다양한 프롬프트 입력\n",
    "prompts = [\n",
    "    'a cat playing piano',\n",
    "    'a dog surfing on beach', \n",
    "    'an elephant painting',\n",
    "    'a penguin wearing a tuxedo',\n",
    "    'a giraffe drinking coffee',\n",
    "    'a panda doing yoga',\n",
    "    'a monkey coding on computer',\n",
    "    'a rabbit cooking in kitchen',\n",
    "    'a lion reading a book'\n",
    "]\n",
    "\n",
    "# 이미지 생성\n",
    "images = pipe(prompts).images\n",
    "\n",
    "# 3x3 그리드로 이미지 출력\n",
    "grid = image_grid(images, rows=3, cols=3)\n",
    "grid\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 이미지를 생성하다 보면 RuntimeError: CUDA out of memory.가 나올 때가 있을 거에요. 그럴 때는 아래의 코드를 사용해 보세요.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리를 지우는 코드입니다. \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image-to-Image Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mage-to-Image Generation은 프롬프트뿐 아니라 이미지를 입력으로 넣으면 다른 이미지로 변형시켜 주는 기능입니다. 앞의 영상에서 언급되었던 'AI야 우리 딸 아이 그림 좀 손봐줘'가 이 기능을 사용한 것이죠.\n",
    "\n",
    "Diffusers에는 이 기능이 포함된 파이프라인을 이미 만들어 두었기 때문에 파이프라인을 불러오기만 하면 쉽게 모델을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-to-Image Generation 파이프라인 불러오기    \n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "device = \"cuda\"\n",
    "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 하나의 이미지 생성하기  \n",
    "\n",
    "사용할 이미지를 불러옵시다.\n",
    "\n",
    "이미지를 클라우드에 올려서 사용해도 되고, 아래의 코드와 같이 인터넷에 있는 이미지를 가져와도 좋습니다. 여기서는 허깅페이스에서 제공하는 기본 이미지를 사용해 볼게요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "\n",
    "# url 호출하기\n",
    "response = requests.get(url)\n",
    "\n",
    "# 이미지 열기\n",
    "init_img = Image.open(BytesIO(response.content)).convert(\"RGB\")  # 이미지를 메모리로 읽어와서 RGB로 변경합니다. \n",
    "init_img = init_img.resize((768, 512))  # 이미지의 크기를 조절합니다. \n",
    "init_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 클라우드에 올려 사용하려면 아래 코드의 주석을 풀고 사용해 보세요.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_img = Image.open(\"/content/drive/MyDrive/diffusers/image.png\", mode = 'r')\n",
    "init_img = init_img.resize((768, 512)) \n",
    "init_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러온 이미지를 내가 원하는 스타일로 변경할 수 있도록 프롬프트를 작성해 주세요.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A fantasy landscape, trending on artstation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 생성하고 저장해 보겠습니다. 아래의 코드에는 조절할 수 있는 하이퍼파라미터가 있습니다.\n",
    "\n",
    "- seed: 동일한 입력 문장과 각종 설정을 넣었을 때 동일한 시드 값을 주면 같은 이미지를 생성할 수 있습니다.\n",
    "- strength: 레퍼런스 이미지에서 얼마나 변형할지를 설정하는 하이퍼파라미터로, 값이 커질수록 원본 이미지와 다른 이미지를 생성합니다. 0과 1 사이의 값을 선택할 수 있으며, default는 0.8입니다.\n",
    "- guidance_scale: 얼마나 주어진 프롬프트에 근접한 이미지를 생성할지를 설정하는 하이퍼파라미터로, 값이 커질수록 문자열에 근접한 이미지가 생성되지만 이미지 품질이 떨어질 수 있습니다. default는 7.5입니다.\n",
    "- num_inference_steps: denoising 스텝 수로, 값이 커질수록 고해상도 이미지가 출력되지만 출력되는 시간이 오래 걸립니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(1024)   # 모델을 사용할 때마다 동일한 이미지를 생성하기 위해 seed를 설정합니다.  \n",
    "\n",
    "images = pipe(prompt=prompt, image=init_img, strength=0.75, guidance_scale=7.5).images\n",
    "images[0].save(\"/content/drive/MyDrive/diffusers/fantasy_landscape.png\")\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 하이퍼파라미터를 변경하면서 다양한 이미지를 생성해 보세요. \n",
    "# 다양한 하이퍼파라미터로 이미지 생성해보기\n",
    "# Try generating images with different hyperparameters\n",
    "\n",
    "# 1. strength 값을 높여서 원본 이미지와 더 다른 이미지 생성\n",
    "# 1. Increase strength to generate more different images from original\n",
    "generator = torch.Generator(device=device).manual_seed(1024)\n",
    "images = pipe(prompt=prompt, image=init_img, strength=0.9, guidance_scale=7.5).images\n",
    "images[0].save(\"/content/drive/MyDrive/diffusers/fantasy_landscape_high_strength.png\")\n",
    "images[0]\n",
    "\n",
    "# 2. guidance_scale 값을 높여서 프롬프트에 더 근접한 이미지 생성  \n",
    "# 2. Increase guidance_scale to generate images closer to prompt\n",
    "generator = torch.Generator(device=device).manual_seed(1024)\n",
    "images = pipe(prompt=prompt, image=init_img, strength=0.75, guidance_scale=15).images\n",
    "images[0].save(\"/content/drive/MyDrive/diffusers/fantasy_landscape_high_guidance.png\")\n",
    "images[0]\n",
    "\n",
    "# 3. 다른 시드값으로 다양한 이미지 생성\n",
    "# 3. Generate different images with different seeds\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "images = pipe(prompt=prompt, image=init_img, strength=0.75, guidance_scale=7.5).images\n",
    "images[0].save(\"/content/drive/MyDrive/diffusers/fantasy_landscape_different_seed.png\")\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 여러 개의 이미지 생성하기\n",
    "\n",
    "여러 개의 프롬프트를 사용하여 여러 장의 이미지를 한번에 생성할 수도 있습니다. Text-to-Image Generation에서 사용했던 코드를 사용하였으니 코드를 자유롭게 변경해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 2\n",
    "\n",
    "# 프롬프트 입력\n",
    "prompt = ['A fantasy landscape, trending on artstation'] * num_images\n",
    "\n",
    "# 이미지 생성\n",
    "generator = torch.Generator(device=device).manual_seed(1024)\n",
    "images = pipe(prompt=prompt, image=init_img, strength=0.9, guidance_scale=13.5, num_inference_steps=50, generator=generator).images\n",
    "images\n",
    "\n",
    "# 이미지 출력\n",
    "grid = image_grid(images, rows=1, cols=2)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 이미지를 생성해 보세요. \n",
    "# 다양한 프롬프트로 여러 이미지 생성\n",
    "# Generate multiple images with different prompts\n",
    "prompts = [\n",
    "    \"A magical forest with glowing mushrooms and fairies, digital art\",\n",
    "    \"A futuristic cyberpunk city at night with neon lights\",\n",
    "    \"A peaceful mountain landscape with a lake at sunset\",\n",
    "    \"An underwater scene with coral reefs and tropical fish\"\n",
    "]\n",
    "\n",
    "num_images = len(prompts)\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "# 이미지 생성\n",
    "# Generate images\n",
    "images = pipe(\n",
    "    prompt=prompts,\n",
    "    image=init_img,\n",
    "    strength=0.85,\n",
    "    guidance_scale=10.0,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator\n",
    ").images\n",
    "\n",
    "# 이미지 그리드로 출력\n",
    "# Display images in a grid\n",
    "grid = image_grid(images, rows=2, cols=2)\n",
    "grid\n",
    "\n",
    "# 이미지 저장\n",
    "# Save images\n",
    "for i, image in enumerate(images):\n",
    "    image.save(f\"/content/drive/MyDrive/diffusers/generated_image_{i+1}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
