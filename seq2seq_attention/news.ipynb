{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스기사 요약해보기 프로젝트\n",
    "\n",
    "이 노트북에서는 뉴스기사 데이터셋(news_summary_more.csv)을 활용하여 **추상적 요약**과 **추출적 요약**을 수행합니다.\n",
    "\n",
    "### 프로젝트 목표\n",
    "- **추상적 요약**: seq2seq 모델과 어텐션 메커니즘을 이용하여 뉴스 기사의 본문(`text`)을 입력받아 헤드라인(`headlines`)을 생성합니다.\n",
    "- **추출적 요약**: Summa 라이브러리의 `summarize` 함수를 사용하여 기사 본문에서 핵심 문장을 추출합니다.\n",
    "\n",
    "### 평가 루브릭\n",
    "1. **텍스트 전처리 단계**: 분석, 정제, 정규화, 불용어 제거, 데이터셋 분리, 인코딩 과정이 체계적으로 진행됨.\n",
    "2. **텍스트 요약 모델 학습**: 모델 학습 중 train/validation loss 감소 추세 확인 및 생성 요약문 내 핵심 단어 포함 확인.\n",
    "3. **추출적 요약과 추상적 요약 비교**: 두 요약 결과를 문법 완성도 및 핵심 단어 포함 측면에서 비교 분석.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import nltk\n",
    "import tensorflow\n",
    "import summa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 다운로드\n",
    "data = pd.read_csv('C:\\\\Users\\\\User\\\\.kaggle\\\\news_summary_more.csv', nrows=100000)\n",
    "print('전체 샘플수 :', len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95921</th>\n",
       "      <td>I'll run out of here, jokes Karan on hearing K...</td>\n",
       "      <td>At a recent event, filmmaker Karan Johar jokin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51104</th>\n",
       "      <td>Pentagon suggests countering cyber attacks wit...</td>\n",
       "      <td>A policy drafted by the US Department of Defen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67667</th>\n",
       "      <td>Anti-ISIS 'sheikh of snipers' killed in Iraq</td>\n",
       "      <td>A veteran fighter known as \"sheikh of snipers\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52941</th>\n",
       "      <td>Rohit Sharma tries trolling Chahal, gets troll...</td>\n",
       "      <td>Indian cricketer Rohit Sharma took to Instagra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61485</th>\n",
       "      <td>Pay lawyers after approval: Delhi Dy CM tells ...</td>\n",
       "      <td>Delhi Deputy CM Manish Sisodia has directed al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70915</th>\n",
       "      <td>40,000 people affected due to flash floods in ...</td>\n",
       "      <td>Nearly 40,000 people in Assam's Sonitpur were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63980</th>\n",
       "      <td>Man flies 8,000 ft above S Africa using 100 he...</td>\n",
       "      <td>A British adventurer Tom Morgan flew 8,000 fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58429</th>\n",
       "      <td>Film industry today has become a \"sabzi mandi\"...</td>\n",
       "      <td>Veteran actor Dharmendra has said that the fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49387</th>\n",
       "      <td>Ambani's Reliance Industries to invest â¹2,50...</td>\n",
       "      <td>Reliance Industries Chairman Mukesh Ambani has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76182</th>\n",
       "      <td>Hockey turf in Rajasthan cut up, being used as...</td>\n",
       "      <td>Pieces of a hockey artificial turf, known as A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "95921  I'll run out of here, jokes Karan on hearing K...   \n",
       "51104  Pentagon suggests countering cyber attacks wit...   \n",
       "67667       Anti-ISIS 'sheikh of snipers' killed in Iraq   \n",
       "52941  Rohit Sharma tries trolling Chahal, gets troll...   \n",
       "61485  Pay lawyers after approval: Delhi Dy CM tells ...   \n",
       "70915  40,000 people affected due to flash floods in ...   \n",
       "63980  Man flies 8,000 ft above S Africa using 100 he...   \n",
       "58429  Film industry today has become a \"sabzi mandi\"...   \n",
       "49387  Ambani's Reliance Industries to invest â¹2,50...   \n",
       "76182  Hockey turf in Rajasthan cut up, being used as...   \n",
       "\n",
       "                                                    text  \n",
       "95921  At a recent event, filmmaker Karan Johar jokin...  \n",
       "51104  A policy drafted by the US Department of Defen...  \n",
       "67667  A veteran fighter known as \"sheikh of snipers\"...  \n",
       "52941  Indian cricketer Rohit Sharma took to Instagra...  \n",
       "61485  Delhi Deputy CM Manish Sisodia has directed al...  \n",
       "70915  Nearly 40,000 people in Assam's Sonitpur were ...  \n",
       "63980  A British adventurer Tom Morgan flew 8,000 fee...  \n",
       "58429  Veteran actor Dharmendra has said that the fil...  \n",
       "49387  Reliance Industries Chairman Mukesh Ambani has...  \n",
       "76182  Pieces of a hockey artificial turf, known as A...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기 (추상적 요약)\n",
    "\n",
    "데이터셋은 기사의 본문(`text`)과 헤드라인(`headlines`) 열로 구성되어 있습니다. \n",
    "이 단계에서는 중복/Null 제거, 텍스트 정규화, 불용어 제거, 길이 제한, 시작/종료 토큰 추가, 그리고 학습/테스트 데이터 분리를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 각 텍스트별 등장 횟수를 계산하여, 1회보다 많이 등장한 항목을 확인\n",
    "duplicate_counts = data['text'].value_counts()\n",
    "duplicates = duplicate_counts[duplicate_counts > 1]\n",
    "print(\"중복된 텍스트와 해당 중복 횟수:\")\n",
    "print(duplicates)\n",
    "\n",
    "# 2. 중복된 행 전체를 출력 (keep=False 옵션으로 모든 중복 행을 반환)\n",
    "duplicate_rows = data[data.duplicated(subset=['text'], keep=False)]\n",
    "print(\"\\n중복된 뉴스 기사 전체:\")\n",
    "print(duplicate_rows.sort_values(by='text'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 41)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=data\n",
    "# Check for duplicates in each individual column ('headlines' and 'text')\n",
    "duplicates_headlines = df.duplicated(subset=['headlines']).sum()\n",
    "duplicates_text = df.duplicated(subset=['text']).sum()\n",
    "\n",
    "duplicates_headlines, duplicates_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
      "headlines 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n"
     ]
    }
   ],
   "source": [
    "print('text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
    "print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null 값 확인:\n",
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 중복 및 Null 값 제거\n",
    "data.drop_duplicates(subset=['text'], inplace=True)\n",
    "# data.dropna(inplace=True)\n",
    "print('Null 값 확인:')\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "\n",
    "    # 불용어 제거 (text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (healines)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13992\\2321396210.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 샘플:\n",
      "                                                text  \\\n",
      "0  saurav kant alumnus upgrad iiit pg program mac...   \n",
      "1  kunal shah credit card bill payment platform c...   \n",
      "2  new zealand defeated india wickets fourth odi ...   \n",
      "3  aegon life iterm insurance plan customers enjo...   \n",
      "4  speaking sexual harassment allegations rajkuma...   \n",
      "\n",
      "                                           headlines  \n",
      "0  upgrad learner switches to career in ml al wit...  \n",
      "1  delhi techie wins free food from swiggy for on...  \n",
      "2  new zealand end rohit sharma led india match w...  \n",
      "3  aegon life iterm insurance plan helps customer...  \n",
      "4  have known hirani for yrs what if metoo claims...  \n"
     ]
    }
   ],
   "source": [
    "# 텍스트와 헤드라인 전처리\n",
    "clean_text = [preprocess_sentence(s) for s in data['text']]\n",
    "clean_headlines = [preprocess_sentence(s, remove_stopwords=False) for s in data['headlines']]\n",
    "\n",
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_headlines\n",
    "\n",
    "# 빈 문자열을 NaN으로 치환 후 제거\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "print('전처리 후 샘플:')\n",
    "print(data[['text', 'headlines']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 후 emptied 된 샘플 확인 후 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines         0\n",
       "text              0\n",
       "decoder_input     0\n",
       "decoder_target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 27105\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연산 속도 최적화를 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# 병렬 처리를 위한 함수\n",
    "def parallel_apply(texts, func, remove_stopwords=True):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        result = p.starmap(func, [(s, remove_stopwords) for s in texts])\n",
    "    return result\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "    sentence = re.sub('\"', '', sentence)\n",
    "    sentence = re.sub(r\"'s\\b\", \"\", sentence)\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if word not in stopwords.words('english') if len(word) > 1)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "\n",
    "# 전처리 수행\n",
    "data['clean_text'] = parallel_apply(data['text'], preprocess_sentence)\n",
    "data['clean_headlines'] = parallel_apply(data['headlines'], preprocess_sentence, remove_stopwords=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 과정 전후의 샘플 수의 급격한 차이에 대한 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 전후 샘플의 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = data['text'].sample(1000, random_state=42)  # 원본 텍스트\n",
    "processed_texts = original_texts.apply(preprocess_sentence)  # 전처리 적용\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "original_tfidf = vectorizer.fit_transform(original_texts)\n",
    "processed_tfidf = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "original_similarity = cosine_similarity(original_tfidf)\n",
    "processed_similarity = cosine_similarity(processed_tfidf)\n",
    "\n",
    "# 유사도 평균 비교\n",
    "original_similarity_mean = np.mean(original_similarity)\n",
    "processed_similarity_mean = np.mean(processed_similarity)\n",
    "\n",
    "original_similarity_mean, processed_similarity_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트 길이 - 최소: 1 최대: 60 평균: 35.09968483123221\n",
      "헤드라인 길이 - 최소: 1 최대: 16 평균: 9.299532330215534\n",
      "최대 길이 조건 후 샘플 수: 27105\n",
      "                                            headlines  \\\n",
      "19  odisha cm patnaik controls mining mafia union ...   \n",
      "21  isro unveils bengaluru centre for manned space...   \n",
      "22              killed injured in saudi arabia floods   \n",
      "29  seat cushions from missing plane carrying foot...   \n",
      "36  agustawestland scam accused rajiv saxena extra...   \n",
      "\n",
      "                                        decoder_input  \\\n",
      "19  sostoken odisha cm patnaik controls mining maf...   \n",
      "21  sostoken isro unveils bengaluru centre for man...   \n",
      "22     sostoken killed injured in saudi arabia floods   \n",
      "29  sostoken seat cushions from missing plane carr...   \n",
      "36  sostoken agustawestland scam accused rajiv sax...   \n",
      "\n",
      "                                       decoder_target  \n",
      "19  odisha cm patnaik controls mining mafia union ...  \n",
      "21  isro unveils bengaluru centre for manned space...  \n",
      "22     killed injured in saudi arabia floods eostoken  \n",
      "29  seat cushions from missing plane carrying foot...  \n",
      "36  agustawestland scam accused rajiv saxena extra...  \n"
     ]
    }
   ],
   "source": [
    "# 데이터 길이 분포 확인 (필요시 시각화)\n",
    "text_len = data['text'].apply(lambda x: len(x.split()))\n",
    "headlines_len = data['headlines'].apply(lambda x: len(x.split()))\n",
    "print('텍스트 길이 - 최소:', text_len.min(), '최대:', text_len.max(), '평균:', text_len.mean())\n",
    "print('헤드라인 길이 - 최소:', headlines_len.min(), '최대:', headlines_len.max(), '평균:', headlines_len.mean())\n",
    "\n",
    "# 최대 길이 제한 (추상적 요약을 위한) \n",
    "text_max_len = 50\n",
    "headlines_max_len = 8\n",
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]\n",
    "print('최대 길이 조건 후 샘플 수:', len(data))\n",
    "\n",
    "# 시작 토큰과 종료 토큰 추가\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x: 'sostoken ' + x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x: x + ' eostoken')\n",
    "\n",
    "print(data[['headlines', 'decoder_input', 'decoder_target']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 수: 21684\n",
      "테스트 샘플 수: 5421\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리 및 셔플\n",
    "import numpy as np\n",
    "\n",
    "encoder_input = np.array(data['text'])\n",
    "decoder_input = np.array(data['decoder_input'])\n",
    "decoder_target = np.array(data['decoder_target'])\n",
    "\n",
    "indices = np.arange(len(encoder_input))\n",
    "np.random.shuffle(indices)\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_val = int(len(encoder_input) * 0.2)\n",
    "encoder_input_train = encoder_input[:-n_val]\n",
    "decoder_input_train = decoder_input[:-n_val]\n",
    "decoder_target_train = decoder_target[:-n_val]\n",
    "encoder_input_test = encoder_input[-n_val:]\n",
    "decoder_input_test = decoder_input[-n_val:]\n",
    "decoder_target_test = decoder_target[-n_val:]\n",
    "\n",
    "print('훈련 샘플 수:', len(encoder_input_train))\n",
    "print('테스트 샘플 수:', len(encoder_input_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 인코더 텍스트 정수 인코딩\n",
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab)\n",
    "src_tokenizer.fit_on_texts(encoder_input_train)\n",
    "encoder_input_train_seq = src_tokenizer.texts_to_sequences(encoder_input_train)\n",
    "encoder_input_test_seq = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "encoder_input_train_seq = pad_sequences(encoder_input_train_seq, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test_seq = pad_sequences(encoder_input_test_seq, maxlen=text_max_len, padding='post')\n",
    "\n",
    "# 디코더 텍스트 정수 인코딩\n",
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab)\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "decoder_input_train_seq = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
    "decoder_input_test_seq = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_train_seq = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_target_test_seq = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "decoder_input_train_seq = pad_sequences(decoder_input_train_seq, maxlen=headlines_max_len+1, padding='post')\n",
    "decoder_input_test_seq = pad_sequences(decoder_input_test_seq, maxlen=headlines_max_len+1, padding='post')\n",
    "decoder_target_train_seq = pad_sequences(decoder_target_train_seq, maxlen=headlines_max_len+1, padding='post')\n",
    "decoder_target_test_seq = pad_sequences(decoder_target_test_seq, maxlen=headlines_max_len+1, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원문 encoder_input_train 단어집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab)\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 어텐션 메커니즘을 사용한 seq2seq 모델 구성 (추상적 요약)\n",
    "\n",
    "인코더-디코더 구조에 어텐션 메커니즘을 추가한 seq2seq 모델을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 어텐션 레이어\u001b[39;00m\n\u001b[0;32m     20\u001b[0m attn_layer \u001b[38;5;241m=\u001b[39m Attention()\n\u001b[1;32m---> 21\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m decoder_concat \u001b[38;5;241m=\u001b[39m Concatenate(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)([decoder_outputs, attn_out])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 출력 레이어\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "enc_emb = Embedding(src_vocab, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(headlines_max_len+1,))\n",
    "dec_emb = Embedding(tar_vocab, latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# 어텐션 레이어\n",
    "attn_layer = Attention()\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attn_out])\n",
    "\n",
    "# 출력 레이어\n",
    "decoder_dense = Dense(tar_vocab, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 (에포크 수는 상황에 맞게 조정)\n",
    "history = model.fit([encoder_input_train_seq, decoder_input_train_seq], \n",
    "                    decoder_target_train_seq[..., np.newaxis], \n",
    "                    batch_size=64, epochs=5, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
    "\n",
    "학습된 모델을 이용해 테스트 데이터의 일부에 대해 요약을 생성하고, 실제 헤드라인과 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: 테스트 데이터의 첫 번째 샘플에 대해 예측 수행\n",
    "preds = model.predict([encoder_input_test_seq, decoder_input_test_seq])\n",
    "predicted_seq = preds.argmax(axis=-1)\n",
    "\n",
    "# 토크나이저의 인덱스를 단어로 매핑하기 위한 역 딕셔너리 생성\n",
    "reverse_tar_word_index = {v: k for k, v in tar_tokenizer.word_index.items()}\n",
    "\n",
    "def decode_sequence(seq):\n",
    "    return ' '.join([reverse_tar_word_index.get(i, '') for i in seq if i != 0])\n",
    "\n",
    "print('실제 헤드라인:', decoder_target_test[0])\n",
    "print('생성된 요약:', decode_sequence(predicted_seq[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Summa를 이용한 추출적 요약\n",
    "\n",
    "Summa 라이브러리의 `summarize` 함수를 사용하여, 기사 본문(`text`)에서 핵심 내용을 추출합니다.\n",
    "\n",
    "※ 추출적 요약은 원문만을 사용하므로, 추상적 요약과 별도로 비교할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize\n",
    "\n",
    "# 테스트 데이터의 첫 번째 기사를 대상으로 추출적 요약 수행\n",
    "sample_article = encoder_input_test[0]\n",
    "print('원문:', sample_article)\n",
    "extractive_summary = summarize(sample_article)\n",
    "print('추출적 요약:', extractive_summary if extractive_summary else '요약 결과가 충분하지 않습니다.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 평가: 프로젝트 평가 루브릭 체크\n",
    "\n",
    "아래 항목들을 통해 프로젝트의 각 단계가 평가 루브릭에 부합하는지 확인합니다.\n",
    "\n",
    "[] 1. **텍스트 전처리 단계**:\n",
    "   - 중복 및 Null 제거, 정규화, 불용어 제거, 데이터셋 분리, 정수 인코딩까지 체계적으로 진행됨.\n",
    "\n",
    "[] 2. **텍스트 요약 모델 학습**:\n",
    "   - 모델 학습 과정에서 train/validation loss의 감소 추세를 확인할 수 있으며,\n",
    "     생성된 요약문에 핵심 단어들이 포함됨.\n",
    "\n",
    "[] 3. **추출적 요약과 추상적 요약 비교**:\n",
    "   - Summa를 이용한 추출적 요약과 seq2seq 기반 추상적 요약 결과를 비교 분석할 수 있음.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
